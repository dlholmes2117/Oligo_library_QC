{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0988b066-180b-4cf8-b5d3-dc7e56cc64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import os\n",
    "from collections import Counter\n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756256a-f85e-4750-a0dc-338db6062b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory with paired read data\n",
    "samples_dirs = ['merged/merged']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21697510-c984-43e9-a712-3f8e5f41648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of sequencing paths\n",
    "sequencing_path_list_barcodes = []\n",
    "for samples_dir in samples_dirs:\n",
    "    sequencing_path_list_barcodes = sequencing_path_list_barcodes + glob(os.path.join(samples_dir,'*.assembled.fastq'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c6f9c-8c7c-42f9-8992-1b1996e94b32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import reads and show the total and unique read counts\n",
    "barcodes_now = {}\n",
    "for (i, filename) in enumerate(sequencing_path_list_barcodes):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        barcodes = [line.rstrip() for line in lines[1::4]]\n",
    "    f.close()\n",
    "    barcodes_now[os.path.basename(filename).split('.')[0]] = barcodes\n",
    "\n",
    "\n",
    "# sort keys\n",
    "oligo_subpools = list(barcodes_now.keys())\n",
    "oligo_subpools.sort()\n",
    "    \n",
    "#read count\n",
    "read_count = [len(s) for s in [barcodes_now[key] for key in oligo_subpools]] \n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(18.5, 10.5, forward=True)\n",
    "plt.bar(oligo_subpools, read_count, color='g')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Total Reads')\n",
    "plt.show()\n",
    "\n",
    "#unique reads\n",
    "barcodes_unique = {}\n",
    "barcodes_unique_count = {}\n",
    "barcodes_freq = {}\n",
    "\n",
    "for key in oligo_subpools:\n",
    "    barcodes_unique[key] = list(set(barcodes_now[key]))\n",
    "    barcodes_unique_count[key] = Counter(barcodes_now[key])\n",
    "    barcodes_freq[key] = {}\n",
    "    totalreads = len(barcodes_now[key])\n",
    "    for item, count in barcodes_unique_count[key].items():\n",
    "        barcodes_freq[key][item] = count/totalreads\n",
    "\n",
    "#unique read count\n",
    "unique_read_count = [len(s) for s in [barcodes_unique[key] for key in oligo_subpools]] \n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(18.5, 10.5, forward=True)\n",
    "plt.bar(oligo_subpools, unique_read_count, color='b')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Unique Reads')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c957e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cut-off detection from the previous cell to the first 10 samples in barcodes_unique_count to see if it is working\n",
    "sample_keys = list(barcodes_unique_count.keys())[:10]\n",
    "cutoffs = {}\n",
    "barcodes_real = {}\n",
    "for key in sample_keys:\n",
    "    print('\\nProcessing', key)\n",
    "    values = np.array(list(barcodes_unique_count[key].values()), dtype=float)\n",
    "    # Filter out very small counts\n",
    "    filtered_values = values[values >= 10**1]\n",
    "    bins = 10**np.linspace(1, 4, 100)\n",
    "    hist_counts, bin_edges = np.histogram(filtered_values, bins=bins)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    window = 10\n",
    "    y_smooth = np.convolve(hist_counts, np.ones(window)/window, mode='same')\n",
    "    peaks, _ = find_peaks(y_smooth)\n",
    "    # filter peaks by minimum smoothed height\n",
    "    min_peak_height = 1\n",
    "    filtered_peaks = peaks[y_smooth[peaks] >= min_peak_height]\n",
    "    if len(filtered_peaks) < 2:\n",
    "        print(f\"  Not enough modes for {key} after height filter; found {len(filtered_peaks)} peaks. Skipping.\")\n",
    "        continue\n",
    "    # pick two right-most peaks\n",
    "    peaks_by_pos = sorted(filtered_peaks, key=lambda p: p, reverse=True)\n",
    "    rightmost_peak = peaks_by_pos[0]\n",
    "    second_rightmost_peak = peaks_by_pos[1]\n",
    "    start = min(rightmost_peak, second_rightmost_peak)\n",
    "    end = max(rightmost_peak, second_rightmost_peak)\n",
    "    y_seg = y_smooth[start:end+1]\n",
    "    min_rel_idx = int(np.argmin(y_seg))\n",
    "    min_val = y_seg[min_rel_idx]\n",
    "    seg_range = y_seg.max() - min_val\n",
    "    rel_frac = 0.05\n",
    "    if seg_range <= 0:\n",
    "        tol = min_val\n",
    "    else:\n",
    "        tol = min_val + rel_frac * seg_range\n",
    "    candidate_rel_idxs = np.where(y_seg <= tol)[0]\n",
    "    if candidate_rel_idxs.size == 0:\n",
    "        candidate_rel_idxs = np.array([min_rel_idx])\n",
    "    blocks = []\n",
    "    if candidate_rel_idxs.size > 0:\n",
    "        block_start = candidate_rel_idxs[0]\n",
    "        block_prev = candidate_rel_idxs[0]\n",
    "        for idx in candidate_rel_idxs[1:]:\n",
    "            if idx == block_prev + 1:\n",
    "                block_prev = idx\n",
    "                continue\n",
    "            else:\n",
    "                blocks.append((block_start, block_prev))\n",
    "                block_start = idx\n",
    "                block_prev = idx\n",
    "        blocks.append((block_start, block_prev))\n",
    "    chosen_block = None\n",
    "    for bstart, bend in blocks:\n",
    "        if bstart <= min_rel_idx <= bend:\n",
    "            chosen_block = (bstart, bend)\n",
    "            break\n",
    "    if chosen_block is None:\n",
    "        lengths = [bend - bstart + 1 for (bstart, bend) in blocks]\n",
    "        chosen_block = blocks[np.argmax(lengths)]\n",
    "    bstart, bend = chosen_block\n",
    "    mid_rel = bstart + (bend - bstart) // 2\n",
    "    valley_index = start + int(mid_rel)\n",
    "    cutoff_value = bin_centers[valley_index]\n",
    "    cutoffs[key] = cutoff_value\n",
    "    # select barcodes with counts >= cutoff_value\n",
    "    barcodes_real[key] = [r for r, s in barcodes_unique_count[key].items() if s >= cutoff_value]\n",
    "    print(f\"  Cut-off for {key}: {cutoff_value}\")\n",
    "    # quick plot for this sample\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(bin_centers, y_smooth, label='Smoothed', color='C2')\n",
    "    plt.plot(bin_centers[filtered_peaks], y_smooth[filtered_peaks], 'ro', label='Modes')\n",
    "    plt.axvspan(bin_centers[start + bstart], bin_centers[start + bend], color='gray', alpha=0.2)\n",
    "    plt.axvline(cutoff_value, color='red', linestyle='--')\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.title(f\"{key} â€” Cut-off: {cutoff_value:.2f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eecdcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cutoff detection to all samples, save filtered sequences per-sample, and write a CSV summary\n",
    "min_read_threshold = 10  # minimum reads considered when building histograms\n",
    "min_peak_height = 1     # peaks with smoothed height below this are ignored\n",
    "bins = 10**np.linspace(1, 4, 100)\n",
    "window = 10\n",
    "outdir = 'filtered_sequences'\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "cutoffs_all = {}\n",
    "barcodes_filtered = {}\n",
    "rows = []\n",
    "for key in barcodes_unique_count.keys():\n",
    "    print('Processing', key)\n",
    "    values = np.array(list(barcodes_unique_count[key].values()), dtype=float)\n",
    "    filtered_values = values[values >= min_read_threshold]\n",
    "    if filtered_values.size == 0:\n",
    "        print(f'  No counts >= {min_read_threshold} for {key}; using cutoff = {min_read_threshold}')\n",
    "        cutoff_value = min_read_threshold\n",
    "    else:\n",
    "        hist_counts, bin_edges = np.histogram(filtered_values, bins=bins)\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        y_smooth = np.convolve(hist_counts, np.ones(window)/window, mode='same')\n",
    "        peaks, _ = find_peaks(y_smooth)\n",
    "        # filter peaks by height\n",
    "        filtered_peaks = peaks[y_smooth[peaks] >= min_peak_height] if peaks.size > 0 else np.array([])\n",
    "        if filtered_peaks.size >= 2:\n",
    "            peaks_by_pos = sorted(filtered_peaks, key=lambda p: p, reverse=True)\n",
    "            rightmost_peak = peaks_by_pos[0]\n",
    "            second_rightmost_peak = peaks_by_pos[1]\n",
    "            start = min(rightmost_peak, second_rightmost_peak)\n",
    "            end = max(rightmost_peak, second_rightmost_peak)\n",
    "            y_seg = y_smooth[start:end+1]\n",
    "            min_rel_idx = int(np.argmin(y_seg))\n",
    "            min_val = y_seg[min_rel_idx]\n",
    "            seg_range = y_seg.max() - min_val\n",
    "            rel_frac = 0.05\n",
    "            if seg_range <= 0:\n",
    "                tol = min_val\n",
    "            else:\n",
    "                tol = min_val + rel_frac * seg_range\n",
    "            candidate_rel_idxs = np.where(y_seg <= tol)[0]\n",
    "            if candidate_rel_idxs.size == 0:\n",
    "                candidate_rel_idxs = np.array([min_rel_idx])\n",
    "            # group into contiguous blocks\n",
    "            blocks = []\n",
    "            block_start = candidate_rel_idxs[0]\n",
    "            block_prev = candidate_rel_idxs[0]\n",
    "            for idx in candidate_rel_idxs[1:]:\n",
    "                if idx == block_prev + 1:\n",
    "                    block_prev = idx\n",
    "                else:\n",
    "                    blocks.append((block_start, block_prev))\n",
    "                    block_start = idx\n",
    "                    block_prev = idx\n",
    "            blocks.append((block_start, block_prev))\n",
    "            # choose block containing the minimum or the longest block\n",
    "            chosen_block = None\n",
    "            for bstart, bend in blocks:\n",
    "                if bstart <= min_rel_idx <= bend:\n",
    "                    chosen_block = (bstart, bend)\n",
    "                    break\n",
    "            if chosen_block is None:\n",
    "                lengths = [bend - bstart + 1 for (bstart, bend) in blocks]\n",
    "                chosen_block = blocks[np.argmax(lengths)]\n",
    "            bstart, bend = chosen_block\n",
    "            mid_rel = bstart + (bend - bstart) // 2\n",
    "            valley_index = start + int(mid_rel)\n",
    "            cutoff_value = bin_centers[valley_index]\n",
    "        else:\n",
    "            # fallback: not enough peaks after filtering -> use minimum threshold\n",
    "            print(f'  Not enough peaks after height filter for {key} (found {filtered_peaks.size}); using cutoff = {min_read_threshold}')\n",
    "            cutoff_value = min_read_threshold\n",
    "    cutoffs_all[key] = cutoff_value\n",
    "    # collect sequences with counts >= cutoff_value\n",
    "    filtered_seqs = [r for r, s in barcodes_unique_count[key].items() if s >= cutoff_value]\n",
    "    # total number of sequences (sum of counts) to the right of the cutoff\n",
    "    total_seqs = sum(s for r, s in barcodes_unique_count[key].items() if s >= cutoff_value)\n",
    "    # number of unique sequences passing the cutoff\n",
    "    n_unique = len(filtered_seqs)\n",
    "    barcodes_filtered[key] = filtered_seqs\n",
    "    # write per-sample sequence file (unique sequences)\n",
    "    outpath = os.path.join(outdir, f\"{key}_filtered.txt\")\n",
    "    with open(outpath, 'w') as outfh:\n",
    "        for seq in filtered_seqs:\n",
    "            outfh.write(seq + '\\n')\n",
    "    rows.append((key, cutoff_value, total_seqs, n_unique))\n",
    "# write CSV summary\n",
    "import csv\n",
    "csv_path = 'cutoff_filtered_counts.csv'\n",
    "with open(csv_path, 'w', newline='') as csvf:\n",
    "    writer = csv.writer(csvf)\n",
    "    writer.writerow(['sample', 'cutoff', 'n_total_sequences', 'n_unique_sequences'])\n",
    "    writer.writerows(rows)\n",
    "print('Wrote per-sample filtered files to', outdir)\n",
    "print('Wrote CSV summary to', csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e75de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import your oligo sheet or sheets and parse them so that they can be merged with the filtered read counts\n",
    "# Load the CSV files without headers\n",
    "master_oligo_df = pd.read_csv('your_first.csv', header=None, names=['name', 'count'])\n",
    "oligos_df = pd.read_csv('your_second.csv', header=None, names=['name', 'count'])\n",
    "\n",
    "# Combine the two DataFrames\n",
    "combined_df = pd.concat([master_oligo_df, oligos_df], ignore_index=True)\n",
    "\n",
    "# Extract the first and second parts of the row names (before the first `_` and second `_`)\n",
    "combined_df['group_1'] = combined_df['name'].str.split('_').str[0]\n",
    "combined_df['group_2'] = combined_df['name'].str.split('_').str[1]\n",
    "\n",
    "# Replace NaN in group_2 with 'None' for clarity\n",
    "combined_df['group_2'] = combined_df['group_2'].fillna('None')\n",
    "\n",
    "# Group by the extracted parts and sum the counts\n",
    "grouped_counts = combined_df.groupby(['group_1', 'group_2'])['count'].sum().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "grouped_counts.columns = ['group_1', 'group_2', 'total_count']\n",
    "\n",
    "# Replace sequences in total_count with the count of rows for each group\n",
    "row_counts = combined_df.groupby(['group_1', 'group_2']).size().reset_index(name='row_count')\n",
    "grouped_counts = grouped_counts.merge(row_counts, on=['group_1', 'group_2'])\n",
    "grouped_counts['total_count'] = grouped_counts['row_count']\n",
    "grouped_counts = grouped_counts.drop(columns=['row_count'])\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(grouped_counts)\n",
    "\n",
    "# Optionally, save the grouped counts to a new CSV file\n",
    "#grouped_counts.to_csv('grouped_counts.csv', index=False)\n",
    "#print(\"Grouped counts saved to 'grouped_counts.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7482ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cutoff_filtered_counts.csv file\n",
    "cutoff_filtered_counts_df = pd.read_csv('cutoff_filtered_counts.csv')\n",
    "# grouped_counts = pd.read_csv('grouped_counts20251120.csv') #if loading from a previously saved file\n",
    "\n",
    "# Extract the first part of the row names (before the first `_`)\n",
    "cutoff_filtered_counts_df['group_1'] = cutoff_filtered_counts_df['sample'].str.split('_').str[0]\n",
    "\n",
    "# Below is an extra step needed if there are exceptions in parsing sample names\n",
    "# # For samples starting with 'AH', extract the text between the first and second `_` and the second and third `_`, then merge them\n",
    "# cutoff_filtered_counts_df['group_2'] = cutoff_filtered_counts_df['sample'].apply(\n",
    "#     lambda x: f\"{x.split('_')[1]}_{x.split('_')[2]}\" if x.startswith('AH') and len(x.split('_')) > 2 else x.split('_')[1]\n",
    "# )\n",
    "\n",
    "# Another extra step for parsing weird sample names\n",
    "# # Replace any instance of \"circRNA\" in group_1 with \"hairpin\"\n",
    "#cutoff_filtered_counts_df['group_1'] = cutoff_filtered_counts_df['group_1'].replace('circRNA', 'hairpin')\n",
    "\n",
    "# Update specific rows for group_2\n",
    "cutoff_filtered_counts_df.loc[250, 'group_2'] = 'current'  # Row 251 (0-indexed)\n",
    "cutoff_filtered_counts_df.loc[251, 'group_2'] = 'full'    # Row 252 (0-indexed)\n",
    "\n",
    "# Merge the two DataFrames based on group_1 and group_2\n",
    "merged_df = pd.merge(cutoff_filtered_counts_df, grouped_counts, on=['group_1', 'group_2'], how='inner')\n",
    "\n",
    "# Calculate fraction_expected and coverage\n",
    "merged_df['fraction_expected'] = merged_df['n_unique_sequences'] / merged_df['total_count']\n",
    "merged_df['coverage'] = merged_df['n_total_sequences'] / merged_df['total_count']\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(merged_df)\n",
    "\n",
    "#Save the .csv \n",
    "merged_df.to_csv('20251120_oligo_lib_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GG_lib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
